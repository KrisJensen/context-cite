{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `ContextCite` with a RAG LangChain example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll show a quick example of how to use `ContextCite` with a RAG chain using the `langchain` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-community langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /mnt/xfs/home/krisgrg/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from context_cite import ContextCiter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, chain\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start with a langchain RAG chain that does not involve `ContextCite`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple RAG chain (without ContextCite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a local `txt` file as our \"database\" to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/xfs/home/krisgrg/conda_envs/camel/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n",
      "  warn_deprecated(\n",
      "Created a chunk of size 1778, which is longer than the specified 1000\n",
      "Created a chunk of size 5233, which is longer than the specified 1000\n",
      "Created a chunk of size 1110, which is longer than the specified 1000\n",
      "Created a chunk of size 1799, which is longer than the specified 1000\n",
      "Created a chunk of size 1051, which is longer than the specified 1000\n",
      "Created a chunk of size 1118, which is longer than the specified 1000\n",
      "Created a chunk of size 1204, which is longer than the specified 1000\n",
      "Created a chunk of size 1897, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    model_kwargs={\"max_length\": 180, \"max_new_tokens\": 180},\n",
    ")\n",
    "\n",
    "\n",
    "loader = TextLoader(\"./assets/transformer.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "system = \"\"\"Answer the user question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"user\", \"{question}\")])\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "MaskedAttention is a variant of the attention mechanism that uses a mask to selectively attend to specific tokens. This can be useful for tasks where the input is noisy or incomplete, such as natural language processing. In 2019, the masked attention mechanism was used in a neural machine translation system that achieved state-of-the-art results on the WMT19 translation task.[4]\n",
      "\n",
      "DeepMind's AlphaGo\n",
      "AlphaGo is a game-playing system developed by DeepMind, a British artificial intelligence company. It uses a deep neural network with attention mechanisms to play Go, a board game. In 2016, AlphaGo defeated the world champion, Lee Sedol, in a series of games. The system used a combination of deep neural networks, recurrent neural networks, and attention mechan\n"
     ]
    }
   ],
   "source": [
    "question = \"What is self-attention?\"\n",
    "qa = chain.invoke(\"What is self-attention?\")\n",
    "a = qa.split(f\"Question: {question}\")[-1]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding in `ContextCite`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, we'll wrap the `prompt` and `llm` in a `ContextCiter` Runnable class. This class will take care of formatting the context and query, as well as run generation with our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "\n",
    "@chain\n",
    "def context_cite(self, context_and_query: dict):\n",
    "    context = context_and_query[\"context\"]\n",
    "    query = context_and_query[\"question\"]\n",
    "    cc = ContextCiter.from_pretrained(CC_MODEL_NAME, context, query)\n",
    "    return cc.get_attributions(as_dataframe=True, top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_chain = {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | context_cite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributed: Self-attention is a mechanism in neural networks that allows for the computation of \"soft\" weights between tokens in parallel, which leads to improved training speed. It is a type of attention mechanism that processes all tokens simultaneously, rather than sequentially, as in recurrent neural networks. The attention mechanism only uses information about other tokens from lower layers, which leads to improved training speed. Self-attention is used in transformer models, which are a type of neural network that has multiple layers and multiple attention heads. Each layer in a transformer model has multiple attention heads, which encode relevance relations that are meaningful to humans. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers. The attention mechanism is directly plugged into the attention mechanism, allowing for pretraining on short context windows and finetuning on longer context windows.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a704095bb0d045e9abc51eb1bacc8357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_fda0b_row0_col0 {\n",
       "  background-color: rgb(80, 180, 80);\n",
       "}\n",
       "#T_fda0b_row1_col0 {\n",
       "  background-color: rgb(127.8287947655293, 200.49805489951257, 127.8287947655293);\n",
       "}\n",
       "#T_fda0b_row2_col0 {\n",
       "  background-color: rgb(138.40599850547682, 205.03114221663293, 138.40599850547682);\n",
       "}\n",
       "#T_fda0b_row3_col0 {\n",
       "  background-color: rgb(169.4000424515271, 218.31430390779735, 169.4000424515271);\n",
       "}\n",
       "#T_fda0b_row4_col0 {\n",
       "  background-color: rgb(186.8231228702037, 225.78133837294445, 186.8231228702037);\n",
       "}\n",
       "#T_fda0b_row5_col0 {\n",
       "  background-color: rgb(190.10661131258058, 227.18854770539167, 190.10661131258058);\n",
       "}\n",
       "#T_fda0b_row6_col0 {\n",
       "  background-color: rgb(207.24930695014973, 234.5354172643499, 207.24930695014973);\n",
       "}\n",
       "#T_fda0b_row7_col0 {\n",
       "  background-color: rgb(253.48048967805923, 254.3487812905968, 253.48048967805923);\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_fda0b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_fda0b_level0_col0\" class=\"col_heading level0 col0\" >Score</th>\n",
       "      <th id=\"T_fda0b_level0_col1\" class=\"col_heading level0 col1\" >Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_fda0b_row0_col0\" class=\"data row0 col0\" >69.257</td>\n",
       "      <td id=\"T_fda0b_row0_col1\" class=\"data row0 col1\" >Since the attention mechanism only uses information about other tokens from lower layers, it can be computed for all tokens in parallel, which leads to improved training speed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_fda0b_row1_col0\" class=\"data row1 col0\" >50.328</td>\n",
       "      <td id=\"T_fda0b_row1_col1\" class=\"data row1 col1\" >The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_fda0b_row2_col0\" class=\"data row2 col0\" >46.142</td>\n",
       "      <td id=\"T_fda0b_row2_col1\" class=\"data row2 col1\" >Many transformer attention heads encode relevance relations that are meaningful to humans.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_fda0b_row3_col0\" class=\"data row3 col0\" >33.876</td>\n",
       "      <td id=\"T_fda0b_row3_col1\" class=\"data row3 col1\" >ALiBi allows pretraining on short context windows, then finetuning on longer context windows.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_fda0b_row4_col0\" class=\"data row4 col0\" >26.981</td>\n",
       "      <td id=\"T_fda0b_row4_col1\" class=\"data row4 col1\" >[1] Transformers, using an attention mechanism, processing all tokens simultaneously, calculated \"soft\" weights between them in successive layers.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_fda0b_row5_col0\" class=\"data row5 col0\" >25.682</td>\n",
       "      <td id=\"T_fda0b_row5_col1\" class=\"data row5 col1\" >Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_fda0b_row6_col0\" class=\"data row6 col0\" >18.897</td>\n",
       "      <td id=\"T_fda0b_row6_col1\" class=\"data row6 col1\" >{\\displaystyle \\left(W_{Q},W_{K},W_{V}\\right)} matrices is called an attention head, and each layer in a transformer model has multiple attention heads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_fda0b_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_fda0b_row7_col0\" class=\"data row7 col0\" >0.601</td>\n",
       "      <td id=\"T_fda0b_row7_col1\" class=\"data row7 col1\" >{\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa57006a980>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is self-attention?\"\n",
    "new_chain.invoke(\"What is self-attention?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
